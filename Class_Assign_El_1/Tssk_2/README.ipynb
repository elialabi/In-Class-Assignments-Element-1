{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 Overview and Implementation Readme\n",
    "\n",
    "## What the task was about\n",
    "The overarching objective of this assignment was to conduct a comprehensive analysis across diverse datasets, covering tasks ranging from dataset characterization to advanced predictive modeling. The tasks were designed to reinforce our understanding of statistical concepts, machine learning algorithms, and exploratory data analysis.\n",
    "\n",
    "## Understanding the Bigger Picture\n",
    "The assignment encompassed several sub-tasks, starting with dataset identification and characterization. Tasks included exploratory and inferential analyses, application of various loss functions, visualizations for comparison, and complex modeling scenarios such as kernel transformations, overfitting, and regularization.\n",
    "\n",
    "## Mathematics and Statistics Involved\n",
    "The project heavily relied on statistical concepts and mathematical techniques. Exploratory analyses involved descriptive statistics, correlation analyses, and visualizations. Predictive modeling utilized regression and classification algorithms, evaluating performance metrics like accuracy, R2, precision, and confusion matrices. for e.g. Multiple regression. y = b1x1 + b2x2 + … + bnxn + c, and # Linear regression. Y=β0+β1X+ϵ\n",
    "where popular throughout my assignment\n",
    "\n",
    "## Implementation Details\n",
    "Libraries such as NumPy, Pandas, Matplotlib, and scikit-learn were instrumental in the implementation. For kernel transformations, Support Vector Machines (SVM) with Radial Basis Function (RBF) kernels were applied. The logic behind choosing specific algorithms aligned with the characteristics of each dataset and task. References were cited when leveraging external resources, ensuring a robust foundation for our analyses.\n",
    "\n",
    "## Outcomes of Each Sub Task\n",
    "Detailed interpretations and explanations accompanied each sub-task's outcomes. This included insights gained from exploratory analyses, the impact of different loss functions on models, the effectiveness of kernel transformations, and the effects of overfitting and regularization.\n",
    "\n",
    "## Challenges and Resolutions\n",
    "I encountered several challenges that required thoughtful consideration and creative problem-solving to ensure the successful completion of each sub-task.\n",
    "\n",
    "1. **Understanding the Data:** The datasets I dealt with were like intricate puzzles, each containing diverse pieces of information. Figuring out how everything fit together posed a challenge. I delved deep into the dataset documentation and employed exploratory data analysis techniques to unravel the underlying patterns and relationships within the data.\n",
    "\n",
    "2. **Picking the Right Tools:** Selecting the most suitable algorithms for regression and classification tasks was a critical decision. This challenge was overcome by conducting a comparative analysis of different algorithms, taking into account factors such as dataset size, linearity, and multicollinearity.\n",
    "\n",
    "3. **Choosing the Right Features:** Selecting the appropriate features for modeling, especially in scenarios of overfitting, required careful consideration. I addressed this challenge through iterative experimentation with different subsets of features, evaluating their impact on model performance.\n",
    "\n",
    "## Acknowledgments and References\n",
    "\n",
    "- **(i)** Iterate over unique levels of 'UsageGroup': [Pandas Documentation](https://pandas.pydata.org/docs/user_guide/groupby.html)\n",
    "- **(i)** Encode categorical variables: [Stack Overflow](https://stackoverflow.com/questions/44474570/sklearn-label-encoding-multiple-columns-pandas-dataframe)\n",
    "- **(ii)** Pruning a Decision Tree: [Stat Infer](https://statinfer.com/204-3-10-pruning-a-decision-tree-in-python/)\n",
    "- **(i)** Syntax for printing unique values: [Stack Overflow](https://stackoverflow.com/questions/27241253/print-the-unique-values-in-every-column-in-a-pandas-dataframe)\n",
    "- **(ii)** Syntax for Converting to numeric: [GitHub](https://github.com/pandas-dev/pandas/issues/17007)\n",
    "- **(iii)** random.normal numpy function: [NumPy Documentation](https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html)\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "1. *Practical Statistics for Data Science*, by Peter Bruce, Andrew Bruce and Peter Gedeck - [Link](https://ebookcentral.proquest.com/lib/UAL/detail.action?docID=6173908#)\n",
    "2. Grus, J. (2019). *Data Science from Scratch: First Principles with Python*. O'Reilly Media, Inc.\n",
    "4. Stephen Marsland: *Machine Learning: An Algorithmic Perspective (Chapman & Hall/CRC Machine Learning & Pattern Recognition)*\n",
    "6. Eldén, L. (2007). *Matrix Methods in Data Mining and Pattern Recognition*. United States: Society for Industrial and Applied Mathematics\n",
    "7. *Introduction to Applied Linear Algebra – Vectors, Matrices, and Least Squares* by Stephen Boyd and Lieven Vandenberghe - [Link](https://amzn.eu/d/3knw6E3)\n",
    "8. Field, A. (2013). *Discovering statistics using (IBM SPSS statistics or R)*. sage.\n",
    "9. Robert Stengle: *Optimal Control and Estimation* - [Link](https://www.amazon.co.uk/Optimal-Control-Estimation-Dover-Mathematics/dp/0486682005)\n",
    "10. Sebastian Thrun, Wolfram Burgard, Dieter Fox: *Probabilistic Robotics* - [Link](https://amzn.eu/d/gS71bM9)\n",
    "\n",
    "\n",
    "\n",
    "**Special thanks to Dr. Kayalvizhi Jayavel for her invaluable assistance and guidance throughout this assignment. Additionally, we express gratitude to Prof. Tim Smith for providing engaging research and data, enriching our learning experience.**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
